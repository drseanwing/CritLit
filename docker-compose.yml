version: "3.8"

services:
  postgres:
    image: pgvector/pgvector:pg16
    container_name: slr_postgres
    environment:
      POSTGRES_USER: slr_user
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: slr_database
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./init-scripts:/docker-entrypoint-initdb.d
    ports:
<<<<<<< HEAD
      - "5432:5432"
=======
      - "7360:5432"
>>>>>>> main
    command:
      - postgres
      - -c
      - shared_buffers=512MB
      - -c
      - work_mem=32MB
      - -c
      - maintenance_work_mem=256MB
      - -c
      - max_connections=100
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U slr_user -d slr_database"]
      interval: 10s
      timeout: 5s
      retries: 5

  n8n:
    image: n8nio/n8n:latest
    container_name: slr_n8n
    environment:
      - N8N_BASIC_AUTH_ACTIVE=true
      - N8N_BASIC_AUTH_USER=${N8N_USER}
      - N8N_BASIC_AUTH_PASSWORD=${N8N_PASSWORD}
      - DB_TYPE=postgresdb
      - DB_POSTGRESDB_HOST=postgres
      - DB_POSTGRESDB_PORT=5432
      - DB_POSTGRESDB_DATABASE=slr_database
      - DB_POSTGRESDB_USER=slr_user
      - DB_POSTGRESDB_PASSWORD=${POSTGRES_PASSWORD}
      - EXECUTIONS_MODE=queue
      - QUEUE_BULL_REDIS_HOST=redis
      - N8N_ENCRYPTION_KEY=${N8N_ENCRYPTION_KEY}
    volumes:
      - n8n_data:/home/node/.n8n
      - ./workflows:/workflows
    ports:
<<<<<<< HEAD
      - "5678:5678"
=======
      - "7361:5678"
>>>>>>> main
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_started

  n8n-worker:
    image: n8nio/n8n:latest
    container_name: slr_n8n_worker
    command: n8n worker
    environment:
      - DB_TYPE=postgresdb
      - DB_POSTGRESDB_HOST=postgres
      - DB_POSTGRESDB_PORT=5432
      - DB_POSTGRESDB_DATABASE=slr_database
      - DB_POSTGRESDB_USER=slr_user
      - DB_POSTGRESDB_PASSWORD=${POSTGRES_PASSWORD}
      - EXECUTIONS_MODE=queue
      - QUEUE_BULL_REDIS_HOST=redis
      - N8N_ENCRYPTION_KEY=${N8N_ENCRYPTION_KEY}
    depends_on:
      - n8n
      - redis

<<<<<<< HEAD
=======
  # n8n-mcp: MCP (Model Context Protocol) server for n8n integration
  # Enables Claude and other AI assistants to interact with n8n workflows via stdio protocol
  # Connect via Docker exec or configure in Claude settings (no HTTP ports needed)
  n8n-mcp:
    image: ghcr.io/czlonkowski/n8n-mcp:latest
    container_name: slr_n8n_mcp
    environment:
      - MCP_MODE=stdio
      - LOG_LEVEL=error
      - DISABLE_CONSOLE_OUTPUT=true
      - N8N_API_URL=http://n8n:5678
      - N8N_API_KEY=${N8N_API_KEY}
      - WEBHOOK_SECURITY_MODE=moderate
    depends_on:
      - n8n
    # Note: No ports exposed - MCP uses stdio protocol
    # Connect via Docker exec or configure in Claude settings

>>>>>>> main
  redis:
    image: redis:7-alpine
    container_name: slr_redis
    volumes:
      - redis_data:/data

  ollama:
    image: ollama/ollama:latest
    container_name: slr_ollama
    volumes:
      - ollama_data:/root/.ollama
    ports:
<<<<<<< HEAD
      - "11434:11434"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
=======
      - "7362:11434"
    # NVIDIA GPU support commented out for testing server compatibility
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]
>>>>>>> main

  i-librarian:
    image: cgrima/i-librarian
    container_name: slr_ilibrarian
    volumes:
      - ilibrarian_data:/app/data
    ports:
<<<<<<< HEAD
      - "8080:80"
=======
      - "7363:80"
>>>>>>> main
    environment:
      - ILIBRARIAN_UPLOAD_MAX_SIZE=100M

volumes:
  postgres_data:
  n8n_data:
  redis_data:
  ollama_data:
  ilibrarian_data:
